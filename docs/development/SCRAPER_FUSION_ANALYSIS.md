# JobSpy + SeekSpider èåˆæ–¹æ¡ˆæ·±åº¦åˆ†æ

> **åˆ†ææ—¶é—´:** 2025-12-16
> **ç›®æ ‡:** è¯„ä¼°ä¸¤ä¸ªé¡¹ç›®èåˆçš„å¯è¡Œæ€§ï¼Œå–é•¿è¡¥çŸ­

---

## ğŸ” æ¶æ„å¯¹æ¯”åˆ†æ

### JobSpy æ¶æ„ï¼ˆè½»é‡çº§ã€æ¨¡å—åŒ–ï¼‰

```
æ¶æ„ç‰¹ç‚¹ï¼š
â”œâ”€â”€ åŸºäºæŠ½è±¡ç±» Scraper çš„æ’ä»¶å¼æ¶æ„
â”œâ”€â”€ æ¯ä¸ªç«™ç‚¹ä¸€ä¸ªç‹¬ç«‹çš„ Scraper å®ç°
â”œâ”€â”€ ç»Ÿä¸€çš„æ•°æ®æ¨¡å‹ï¼ˆPydantic BaseModelï¼‰
â”œâ”€â”€ æ— æ¡†æ¶ä¾èµ–ï¼ˆçº¯ requests + beautifulsoup4ï¼‰
â””â”€â”€ å¹¶å‘æŠ“å–ï¼ˆThreadPoolExecutorï¼‰

æ ¸å¿ƒè®¾è®¡æ¨¡å¼ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Scraper (ABC)                     â”‚
â”‚   â”œâ”€â”€ scrape() â†’ JobResponse        â”‚
â”‚   â””â”€â”€ æ¯ä¸ªç«™ç‚¹ç»§æ‰¿å®ç°               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Indeed(Scraper)                   â”‚
â”‚   LinkedIn(Scraper)                 â”‚
â”‚   Glassdoor(Scraper)                â”‚
â”‚   ZipRecruiter(Scraper)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä»£ç ç¤ºä¾‹:**
```python
class Scraper(ABC):
    def __init__(self, site: Site, proxies: list[str] | str | None = None):
        self.site = site
        self.proxies = proxies

    @abstractmethod
    def scrape(self, scraper_input: ScraperInput) -> JobResponse:
        """å­ç±»å¿…é¡»å®ç°çš„æŠ“å–æ–¹æ³•"""
        pass

class Indeed(Scraper):
    def scrape(self, scraper_input: ScraperInput) -> JobResponse:
        # Indeed ç‰¹å®šå®ç°
        jobs = self._scrape_page(...)
        return JobResponse(jobs=jobs)
```

---

### SeekSpider æ¶æ„ï¼ˆScrapy æ¡†æ¶ã€é‡é‡çº§ï¼‰

```
æ¶æ„ç‰¹ç‚¹ï¼š
â”œâ”€â”€ åŸºäº Scrapy æ¡†æ¶çš„ä¼ ç»Ÿçˆ¬è™«
â”œâ”€â”€ Spider + Item + Pipeline ä¸‰å±‚æ¶æ„
â”œâ”€â”€ ç›´æ¥å†™å…¥ PostgreSQLï¼ˆç´§è€¦åˆï¼‰
â”œâ”€â”€ AI å¢å¼ºåŠŸèƒ½ï¼ˆå¯é€‰ï¼‰
â””â”€â”€ Selenium è‡ªåŠ¨åŒ–ç™»å½•

æ ¸å¿ƒè®¾è®¡æ¨¡å¼ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SeekSpider(scrapy.Spider)         â”‚
â”‚   â”œâ”€â”€ start_requests()              â”‚
â”‚   â”œâ”€â”€ parse()                       â”‚
â”‚   â””â”€â”€ parse_job()                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   SeekspiderItem                    â”‚
â”‚   â””â”€â”€ Scrapy.Item æ•°æ®å®¹å™¨          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   SeekspiderPipeline                â”‚
â”‚   â””â”€â”€ ç›´æ¥å†™å…¥ PostgreSQL            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä»£ç ç¤ºä¾‹:**
```python
class SeekSpider(scrapy.Spider):
    name = "seek"

    def start_requests(self):
        yield scrapy.Request(self.base_url)

    def parse(self, response):
        data = response.json()
        for job in data['data']:
            yield self.parse_job(job)

    def parse_job(self, data):
        item = SeekspiderItem()
        item['job_id'] = data['id']
        return item  # ä¼ é€’ç»™ Pipeline
```

---

## ğŸ’¡ èåˆæ–¹æ¡ˆè®¾è®¡

### æ–¹æ¡ˆ 1: å°† SEEK æ”¹é€ ä¸º JobSpy é£æ ¼ â­â­â­â­â­ **å¼ºçƒˆæ¨è**

**æ ¸å¿ƒæ€è·¯:** ä¿ç•™ JobSpy çš„æ¶æ„ï¼Œæ·»åŠ  SEEK ä½œä¸ºæ–°çš„ Scraper

#### å®ç°æ­¥éª¤

**Step 1: åˆ›å»º SeekScraper ç±»**
```python
# jobspy/seek/__init__.py

from jobspy.model import Scraper, ScraperInput, JobResponse, JobPost, Site
import requests

class Seek(Scraper):
    """SEEK æ¾³æ´²æ±‚èŒç½‘ç«™çˆ¬è™«"""

    def __init__(self, proxies: list[str] | str | None = None,
                 ca_cert: str | None = None,
                 user_agent: str | None = None):
        super().__init__(Site.SEEK, proxies=proxies)

        self.base_url = "https://www.seek.com.au/api/jobsearch/v5/search"
        self.session = requests.Session()
        self.headers = {
            'User-Agent': user_agent or 'Mozilla/5.0...',
            'Accept': 'application/json',
        }

    def scrape(self, scraper_input: ScraperInput) -> JobResponse:
        """å®ç°æŠ½è±¡æ–¹æ³•"""
        jobs = []
        page = 1

        # æ„å»ºæœç´¢å‚æ•°
        params = self._build_params(scraper_input, page)

        while len(jobs) < scraper_input.results_wanted:
            response = self.session.get(
                self.base_url,
                params=params,
                headers=self.headers
            )
            data = response.json()

            # è§£æèŒä½
            for job_data in data.get('data', []):
                job = self._parse_job(job_data)
                jobs.append(job)

            # æ£€æŸ¥åˆ†é¡µ
            if page >= data.get('totalPages', 1):
                break
            page += 1
            params['page'] = page

        return JobResponse(jobs=jobs[:scraper_input.results_wanted])

    def _build_params(self, scraper_input: ScraperInput, page: int):
        """æ„å»º SEEK API å‚æ•°"""
        # ä» SeekSpider æå–çš„é€»è¾‘
        return {
            'siteKey': 'AU-Main',
            'where': f'All {scraper_input.location}',
            'keywords': scraper_input.search_term,
            'page': page,
            'locale': 'en-AU',
            # TODO: æ·»åŠ  Trades èŒä½åˆ†ç±»
        }

    def _parse_job(self, data: dict) -> JobPost:
        """è§£æå•ä¸ªèŒä½ï¼ˆä» SeekSpider ç§»æ¤ï¼‰"""
        return JobPost(
            id=str(data.get('id')),
            title=data.get('title', ''),
            company_name=data.get('advertiser', {}).get('description', ''),
            job_url=f"https://www.seek.com.au/job/{data.get('id')}",
            location=self._parse_location(data),
            description=data.get('teaser', ''),
            # ... å…¶ä»–å­—æ®µ
        )
```

**Step 2: æ³¨å†Œåˆ° JobSpy**
```python
# jobspy/__init__.py

from jobspy.seek import Seek  # æ–°å¢

SCRAPER_MAPPING = {
    Site.LINKEDIN: LinkedIn,
    Site.INDEED: Indeed,
    Site.SEEK: Seek,  # æ–°å¢
    # ...
}
```

**Step 3: æ·»åŠ  SEEK åˆ°æšä¸¾**
```python
# jobspy/model.py

class Site(Enum):
    LINKEDIN = "linkedin"
    INDEED = "indeed"
    SEEK = "seek"  # æ–°å¢
    # ...
```

**Step 4: ä½¿ç”¨æ–¹å¼**
```python
from jobspy import scrape_jobs

# åŒæ—¶æŠ“å– Indeed å’Œ SEEK
jobs = scrape_jobs(
    site_name=["indeed", "seek"],  # æ”¯æŒ SEEKï¼
    search_term="tiler",
    location="Adelaide",
    country_indeed='Australia',
    results_wanted=50
)

# è¿”å›ç»Ÿä¸€çš„ DataFrame
jobs.to_csv("jobs.csv")
```

#### ä¼˜ç‚¹åˆ†æ
âœ… **æ¶æ„ç»Ÿä¸€** - æ‰€æœ‰ç«™ç‚¹ä½¿ç”¨åŒä¸€å¥—æ¥å£
âœ… **ä»£ç å¤ç”¨** - å¤ç”¨ JobSpy çš„å¹¶å‘ã€ä»£ç†ã€æ•°æ®æ¨¡å‹
âœ… **è½»é‡çº§** - æ— éœ€ Scrapyã€Selenium
âœ… **æ˜“ç»´æŠ¤** - æ¨¡å—åŒ–è®¾è®¡ï¼ŒSEEK ç‹¬ç«‹æ–‡ä»¶å¤¹
âœ… **æ˜“æµ‹è¯•** - å•å…ƒæµ‹è¯•å‹å¥½
âœ… **çµæ´»è°ƒç”¨** - å¯å•ç‹¬è°ƒç”¨ SEEKï¼Œä¹Ÿå¯æ··åˆæŠ“å–

#### éœ€è¦ä» SeekSpider ç§»æ¤çš„æ ¸å¿ƒé€»è¾‘
1. âœ… SEEK API ç«¯ç‚¹å’Œå‚æ•°ï¼ˆå·²çŸ¥ï¼‰
2. âœ… èŒä½æ•°æ®è§£æé€»è¾‘ï¼ˆ`parse_job`ï¼‰
3. âš ï¸ Trades èŒä½åˆ†ç±» IDï¼ˆéœ€ç ”ç©¶ï¼‰
4. âŒ Selenium ç™»å½•ï¼ˆéªŒè¯æ˜¯å¦å¿…éœ€ï¼‰
5. âŒ AI åˆ†æï¼ˆä¸éœ€è¦ï¼‰
6. âŒ PostgreSQL ç®¡é“ï¼ˆä¸éœ€è¦ï¼‰

---

### æ–¹æ¡ˆ 2: ä¿ç•™ SeekSpiderï¼Œé€šè¿‡ FastAPI æ¡¥æ¥ âŒ **ä¸æ¨è**

**æ¶æ„:**
```
FastAPI
â”œâ”€â”€ /scrape/jobs?source=indeed
â”‚   â””â”€ è°ƒç”¨ JobSpy
â””â”€â”€ /scrape/jobs?source=seek
    â””â”€ è°ƒç”¨ Scrapy (subprocess)
```

**ç¼ºç‚¹:**
- âŒ ä¸¤å¥—æ¶æ„å¹¶å­˜ï¼Œç»´æŠ¤æˆæœ¬é«˜
- âŒ Scrapy éœ€è¦ subprocess è°ƒç”¨ï¼Œæ€§èƒ½å·®
- âŒ æ•°æ®æ ¼å¼ä¸ç»Ÿä¸€ï¼Œéœ€è¦é¢å¤–è½¬æ¢
- âŒ æ— æ³•å¹¶å‘æŠ“å–å¤šä¸ªç«™ç‚¹

---

### æ–¹æ¡ˆ 3: å®Œå…¨åŸºäº Scrapy é‡å†™ âŒ **ä¸æ¨è**

**éœ€è¦åšçš„äº‹:**
- ç”¨ Scrapy é‡å†™ Indeed çˆ¬è™«
- ç”¨ Scrapy é‡å†™ LinkedIn çˆ¬è™«
- ...

**ç¼ºç‚¹:**
- âŒ å·¥ä½œé‡å·¨å¤§
- âŒ JobSpy å·²ç»å¾ˆæˆç†Ÿï¼Œé‡å†™æ— æ„ä¹‰
- âŒ Scrapy è¿‡äºé‡é‡çº§

---

## ğŸ¯ æ¨èå®æ–½æ–¹æ¡ˆ

### âœ… æœ€ç»ˆæ–¹æ¡ˆï¼šæ–¹æ¡ˆ 1 - å°† SEEK æ”¹é€ ä¸º JobSpy æ’ä»¶

#### å®æ–½è®¡åˆ’

**Phase 1: æœ€å°åŒ–éªŒè¯ï¼ˆä»Šå¤©ï¼Œ2å°æ—¶ï¼‰**
```bash
# 1. åœ¨ JobSpy é¡¹ç›®ä¸­åˆ›å»º seek æ–‡ä»¶å¤¹
mkdir jobspy/seek
touch jobspy/seek/__init__.py
touch jobspy/seek/util.py

# 2. å®ç°åŸºç¡€ SeekScraperï¼ˆä¸å«è¯¦æƒ…é¡µæŠ“å–ï¼‰
#    - åªæŠ“å–æœç´¢ç»“æœé¡µçš„åŸºæœ¬ä¿¡æ¯
#    - éªŒè¯ API è°ƒç”¨å¯è¡Œæ€§

# 3. æµ‹è¯•æŠ“å– 10 æ¡æ•°æ®
python test_seek.py
```

**Phase 2: å®Œæ•´å®ç°ï¼ˆæ˜å¤©ï¼Œ4å°æ—¶ï¼‰**
```bash
# 1. ç ”ç©¶ Trades èŒä½åˆ†ç±» ID
#    - æµè§ˆå™¨è®¿é—® SEEKï¼Œæœç´¢ "tiler"
#    - Network æŠ“åŒ…ï¼Œæ‰¾åˆ° classification å‚æ•°

# 2. å®ç°è¯¦æƒ…é¡µæŠ“å–ï¼ˆå¯é€‰ï¼‰
#    - å‚è€ƒ SeekSpider çš„ _enrich_job_details
#    - ä½¿ç”¨ BeautifulSoup è§£æ

# 3. å®Œå–„æ•°æ®æ˜ å°„
#    - SEEK å­—æ®µ â†’ JobPost æ¨¡å‹

# 4. é›†æˆæµ‹è¯•
#    - åŒæ—¶æŠ“å– Indeed + SEEK
#    - éªŒè¯æ•°æ®æ ¼å¼ä¸€è‡´æ€§
```

**Phase 3: FastAPI åŒ…è£…ï¼ˆåå¤©ï¼Œ2å°æ—¶ï¼‰**
```python
# main.py
from fastapi import FastAPI
from jobspy import scrape_jobs

app = FastAPI()

@app.post("/scrape/jobs")
async def scrape(request: ScrapeRequest):
    df = scrape_jobs(
        site_name=[request.source],  # "indeed" or "seek"
        search_term=" ".join(request.keywords),
        location=f"{request.location}, Australia",
        results_wanted=request.max_results,
        country_indeed='Australia'
    )

    # è½¬æ¢ä¸º .NET æœŸæœ›çš„æ ¼å¼
    jobs = df.to_dict('records')
    return {"jobs": jobs, "total": len(jobs)}
```

---

## ğŸ“Š æ ¸å¿ƒä»£ç å¯¹æ¯”

### æ•°æ®æŠ“å–é€»è¾‘

**JobSpy (Indeed):**
```python
# GraphQL API è°ƒç”¨
response = self.session.post(
    self.api_url,
    json={"query": graphql_query},
    headers=self.headers
)
jobs = response.json()['data']['jobSearch']['results']
```

**SeekSpider (SEEK):**
```python
# REST API è°ƒç”¨
response = requests.get(
    "https://www.seek.com.au/api/jobsearch/v5/search",
    params={'where': 'Adelaide', 'page': 1}
)
jobs = response.json()['data']
```

**ç»“è®º:** ä¸¤è€…éƒ½æ˜¯ HTTP API è°ƒç”¨ï¼ŒSeekSpider ç”šè‡³æ›´ç®€å•ï¼

---

### æ•°æ®æ¨¡å‹æ˜ å°„

| å­—æ®µ | JobSpy (JobPost) | SeekSpider (Item) | æ˜¯å¦å…¼å®¹ |
|------|------------------|-------------------|---------|
| ID | `id: str` | `job_id: str` | âœ… ç›´æ¥æ˜ å°„ |
| æ ‡é¢˜ | `title: str` | `job_title: str` | âœ… ç›´æ¥æ˜ å°„ |
| å…¬å¸ | `company_name: str` | `business_name: str` | âœ… ç›´æ¥æ˜ å°„ |
| é“¾æ¥ | `job_url: str` | `url: str` | âœ… ç›´æ¥æ˜ å°„ |
| åœ°ç‚¹ | `location: Location` | `suburb + area` | âš ï¸ éœ€è½¬æ¢ |
| æè¿° | `description: str` | `job_description: str` | âœ… ç›´æ¥æ˜ å°„ |
| è–ªèµ„ | `compensation: Compensation` | `pay_range: str` | âš ï¸ éœ€è§£æ |
| å‘å¸ƒæ—¥æœŸ | `date_posted: date` | `posted_date: str` | âš ï¸ éœ€è§£æ |

**ç»“è®º:** 90% çš„å­—æ®µå¯ä»¥ç›´æ¥æ˜ å°„ï¼Œå°‘æ•°éœ€è¦ç®€å•è½¬æ¢

---

## ğŸ”§ å…·ä½“ç§»æ¤æ­¥éª¤

### ä» SeekSpider æå–çš„æ ¸å¿ƒä»£ç 

**1. API ç«¯ç‚¹å’Œå‚æ•°ï¼ˆç›´æ¥å¤ç”¨ï¼‰**
```python
# ä» SeekSpider/spiders/seek.py ç¬¬ 21-47 è¡Œ
base_url = "https://www.seek.com.au/api/jobsearch/v5/search"

search_params = {
    'siteKey': 'AU-Main',
    'sourcesystem': 'houston',
    'where': 'All Perth WA',  # åŠ¨æ€æ›¿æ¢
    'page': 1,
    'seekSelectAllPages': 'true',
    'classification': '6281',  # âš ï¸ éœ€è¦æ”¹ä¸º Trades
    'include': 'seodata',
    'locale': 'en-AU',
}
```

**2. èŒä½è§£æé€»è¾‘ï¼ˆéœ€é€‚é…ï¼‰**
```python
# ä» SeekSpider/spiders/seek.py ç¬¬ 159-193 è¡Œ
def _parse_job(self, data: dict) -> JobPost:
    # æå–åŸºæœ¬ä¿¡æ¯
    job_id = data['id']
    title = data.get('title', '')
    company = data.get('advertiser', {}).get('description', '')

    # æå–åœ°ç‚¹
    location_data = data.get('locations', [{}])[0]
    location = Location(
        city=location_data.get('label', '').split(',')[0],
        state='SA',  # ä» location è§£æ
        country=Country.AUSTRALIA
    )

    # æå–è–ªèµ„ï¼ˆéœ€è¦è§£æå­—ç¬¦ä¸²ï¼‰
    salary_label = data.get('salaryLabel', '')
    compensation = self._parse_salary(salary_label)

    return JobPost(
        id=str(job_id),
        title=title,
        company_name=company,
        location=location,
        compensation=compensation,
        job_url=f"https://www.seek.com.au/job/{job_id}",
        description=data.get('teaser', ''),
        date_posted=self._parse_date(data.get('listingDate'))
    )
```

**3. è¯¦æƒ…é¡µæŠ“å–ï¼ˆå¯é€‰ï¼Œæå‡æ•°æ®è´¨é‡ï¼‰**
```python
# ä» SeekSpider/spiders/seek.py ç¬¬ 195-226 è¡Œ
def _fetch_job_details(self, job_url: str) -> dict:
    """æŠ“å–èŒä½è¯¦æƒ…é¡µï¼ˆå®Œæ•´æè¿°ï¼‰"""
    response = requests.get(job_url, headers=self.headers)
    soup = BeautifulSoup(response.text, 'lxml')

    # æå–å®Œæ•´èŒä½æè¿°
    job_details = soup.find("div", {"data-automation": "jobAdDetails"})
    description = str(job_details) if job_details else None

    # æå–å…¶ä»–è¯¦ç»†ä¿¡æ¯
    location = soup.find("span", {"data-automation": "job-detail-location"})
    work_type = soup.find("span", {"data-automation": "job-detail-work-type"})

    return {
        'description': description,
        'location': location.text if location else None,
        'work_type': work_type.text if work_type else None
    }
```

---

## ğŸš¨ å…³é”®é£é™©ä¸è§£å†³æ–¹æ¡ˆ

### é£é™© 1: Trades èŒä½åˆ†ç±» ID æœªçŸ¥
**å½±å“:** æ— æ³•ç²¾å‡†æœç´¢ Trades èŒä½
**è§£å†³æ–¹æ¡ˆ:**
```bash
# æ–¹æ¡ˆ A: æµè§ˆå™¨æŠ“åŒ…
1. è®¿é—® https://www.seek.com.au
2. æœç´¢ "tiler"
3. æ‰“å¼€ DevTools â†’ Network
4. æŸ¥æ‰¾ /api/jobsearch/v5/search è¯·æ±‚
5. è®°å½• classification å‚æ•°

# æ–¹æ¡ˆ B: å…³é”®è¯æœç´¢ï¼ˆå¦‚æœåˆ†ç±»IDä¸å¯ç”¨ï¼‰
params = {
    'keywords': 'tiler bricklayer',  # å…³é”®è¯æœç´¢
    # ä¸æŒ‡å®š classification
}
```

### é£é™© 2: SEEK æ˜¯å¦éœ€è¦ç™»å½•
**å½±å“:** å¯èƒ½æ— æ³•è®¿é—® API
**è§£å†³æ–¹æ¡ˆ:**
```python
# æµ‹è¯•æœªç™»å½•è®¿é—®
response = requests.get(
    "https://www.seek.com.au/api/jobsearch/v5/search",
    params={'where': 'Adelaide', 'keywords': 'tiler'}
)

if response.status_code == 401:
    # éœ€è¦ç™»å½•ï¼Œä½¿ç”¨ requests æ¨¡æ‹Ÿï¼ˆé¿å… Seleniumï¼‰
    login_response = requests.post(
        "https://www.seek.com.au/oauth/login",
        json={'username': '...', 'password': '...'}
    )
    token = login_response.json()['access_token']
    headers = {'Authorization': f'Bearer {token}'}
else:
    # æ— éœ€ç™»å½•ï¼
    pass
```

### é£é™© 3: æ•°æ®æ ¼å¼å˜åŒ–
**å½±å“:** SEEK æ”¹ API å¯¼è‡´è§£æå¤±è´¥
**è§£å†³æ–¹æ¡ˆ:**
```python
# å¥å£®çš„è§£æé€»è¾‘
def _parse_job(self, data: dict) -> JobPost:
    try:
        job_id = data['id']  # å¿…éœ€å­—æ®µ
    except KeyError:
        raise ParseError("Missing job ID")

    # å¯é€‰å­—æ®µä½¿ç”¨ .get() + é»˜è®¤å€¼
    title = data.get('title', 'Unknown Title')
    company = data.get('advertiser', {}).get('description', 'Unknown Company')

    return JobPost(...)
```

---

## âœ… æœ€ç»ˆç»“è®º

### æ˜¯å¦å¯ä»¥èåˆï¼Ÿ**å¯ä»¥ï¼** â­â­â­â­â­

**èåˆæ–¹å¼:** å°† SeekSpider çš„æ ¸å¿ƒé€»è¾‘ç§»æ¤åˆ° JobSpy æ¶æ„

**å·¥ä½œé‡è¯„ä¼°:**
- **ä»£ç è¡Œæ•°:** ~200 è¡Œï¼ˆSeekSpider æ ¸å¿ƒé€»è¾‘ï¼‰
- **å¼€å‘æ—¶é—´:** 6-8 å°æ—¶
- **éš¾åº¦:** â­â­â­ (ä¸­ç­‰)

**æ”¶ç›Š:**
- âœ… ç»Ÿä¸€æ¶æ„ï¼Œæ˜“ç»´æŠ¤
- âœ… æ”¯æŒå¹¶å‘æŠ“å– Indeed + SEEK
- âœ… æ— éœ€ Scrapyã€Selenium
- âœ… ä»£ç é‡å‡å°‘ 70%

**é£é™©:**
- âš ï¸ Trades åˆ†ç±» ID éœ€è¦ç ”ç©¶ï¼ˆ1å°æ—¶ï¼‰
- âš ï¸ å¯èƒ½éœ€è¦ç™»å½•ï¼ˆå¾…éªŒè¯ï¼‰
- âš ï¸ API ç¨³å®šæ€§ï¼ˆSEEK æœªå…¬å¼€æ–‡æ¡£ï¼‰

---

## ğŸ“… å®æ–½å»ºè®®

**ä»Šå¤©ï¼ˆ2å°æ—¶ï¼‰:**
1. åˆ›å»º `jobspy/seek/` æ–‡ä»¶å¤¹
2. å®ç°åŸºç¡€ SeekScraperï¼ˆæ— è¯¦æƒ…é¡µï¼‰
3. æµ‹è¯•æŠ“å– 10 æ¡ Trades èŒä½

**æ˜å¤©ï¼ˆ4å°æ—¶ï¼‰:**
1. ç ”ç©¶ Trades åˆ†ç±» ID
2. å®Œå–„æ•°æ®è§£æ
3. æ·»åŠ è¯¦æƒ…é¡µæŠ“å–ï¼ˆå¯é€‰ï¼‰
4. é›†æˆæµ‹è¯•

**åå¤©ï¼ˆ2å°æ—¶ï¼‰:**
1. FastAPI åŒ…è£…
2. ä¸ .NET å¯¹æ¥æµ‹è¯•
3. å®Œæ•´æ•°æ®æµéªŒè¯

**æ€»è€—æ—¶:** ~8 å°æ—¶ï¼ˆå®Œæˆ V1 MVPï¼‰

---

**ä¸‹ä¸€æ­¥:** ä½ æƒ³ç°åœ¨å¼€å§‹å®æ–½å—ï¼Ÿæˆ‘å¯ä»¥å¸®ä½ ï¼š
1. Fork JobSpy é¡¹ç›®ï¼ˆåœ¨ scrape-api-research åŸºç¡€ä¸Šä¿®æ”¹ï¼‰
2. åˆ›å»º SeekScraper éª¨æ¶
3. æµ‹è¯• SEEK API æ˜¯å¦å¯è®¿é—®ï¼ˆéªŒè¯ç™»å½•éœ€æ±‚ï¼‰
