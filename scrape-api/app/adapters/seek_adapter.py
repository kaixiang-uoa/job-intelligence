"""
SEEK èŒä½æ•°æ®é€‚é…å™¨

ä½¿ç”¨ SEEK å†…éƒ¨ GraphQL API è·å–èŒä½æ•°æ®
"""

import logging
import requests
from typing import List, Optional, Dict, Any
from datetime import datetime

from app.models.job_posting_dto import JobPostingDTO, ScrapeRequest, PlatformEnum
from app.adapters.base_adapter import BaseJobAdapter
from app.utils.location_parser import parse_location
from app.utils.trade_extractor import extract_trade
from app.utils.employment_type import normalize_employment_type
from app.utils.salary_parser import parse_salary_range
from app.utils.html_cleaner import clean_html
from app.exceptions import (
    ScraperNetworkError,
    ScraperTimeoutError,
    ScraperDataError,
    ScraperValidationError,
    ScraperParsingError,
    PlatformException,
    classify_http_error
)

logger = logging.getLogger(__name__)


class SeekAdapter(BaseJobAdapter):
    """
    SEEK èŒä½æ•°æ®é€‚é…å™¨

    ç»§æ‰¿è‡ª BaseJobAdapterï¼Œå®ç° scrape() æ–¹æ³•
    ä½¿ç”¨ SEEK å†…éƒ¨ GraphQL API è·å–èŒä½æ•°æ®
    """

    def __init__(self):
        """
        åˆå§‹åŒ– SEEK é€‚é…å™¨

        é…ç½®:
            - API ç«¯ç‚¹: SEEK GraphQL API
            - Headers: å¿…éœ€çš„è¯·æ±‚å¤´ï¼ˆUser-Agent, seek-request-brand ç­‰ï¼‰
            - GraphQL Query: èŒä½æœç´¢æŸ¥è¯¢æ¨¡æ¿
        """
        super().__init__()

        # SEEK REST API ç«¯ç‚¹ï¼ˆå†…éƒ¨ APIï¼‰
        self.api_url = "https://www.seek.com.au/api/jobsearch/v5/search"

        # å¿…éœ€çš„è¯·æ±‚å¤´
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "application/json",
        }

        logger.info("SeekAdapter initialized")

    @property
    def platform_name(self) -> str:
        """å¹³å°åç§°"""
        return "seek"

    def scrape(self, request: ScrapeRequest) -> List[JobPostingDTO]:
        """
        æŠ“å– SEEK èŒä½æ•°æ®

        Args:
            request: çˆ¬å–è¯·æ±‚å‚æ•°ï¼ˆåŒ…å« keywords, location, max_resultsï¼‰

        Returns:
            List[JobPostingDTO]: æ ‡å‡†åŒ–çš„èŒä½åˆ—è¡¨

        Raises:
            ValueError: å‚æ•°æ— æ•ˆ
            requests.RequestException: API è°ƒç”¨å¤±è´¥
        """
        # éªŒè¯è¯·æ±‚
        self.validate_request(request)

        # æå–å‚æ•°
        keywords = request.keywords
        location = request.location
        results_wanted = request.max_results if request.max_results else 50

        if results_wanted < 1 or results_wanted > 50:
            logger.warning(f"results_wanted={results_wanted} è¶…å‡ºèŒƒå›´ï¼Œè°ƒæ•´ä¸º 50")
            results_wanted = 50

        logger.info(f"å¼€å§‹æŠ“å– SEEK èŒä½: keywords={keywords}, location={location}, results_wanted={results_wanted}")

        try:
            # 1. æ„å»º URL å‚æ•°
            params = self._build_params(keywords, location, results_wanted)

            # 2. è°ƒç”¨ SEEK API
            data = self._call_seek_api(params)

            # 3. æå–èŒä½åˆ—è¡¨
            jobs_data = data.get("data", [])
            total_count = data.get("totalCount", len(jobs_data))

            logger.info(f"SEEK API è¿”å› {len(jobs_data)} ä¸ªèŒä½ï¼ˆæ€»è®¡ {total_count}ï¼‰")

            # 4. è½¬æ¢æ¯ä¸ªèŒä½
            jobs = []
            failed_count = 0
            validation_errors = 0
            parsing_errors = 0

            for job_data in jobs_data:
                try:
                    job_dto = self._transform_job(job_data)
                    if job_dto:
                        jobs.append(job_dto)
                except ScraperValidationError as e:
                    # éªŒè¯é”™è¯¯ï¼ˆç¼ºå°‘å¿…éœ€å­—æ®µï¼‰- è·³è¿‡è¯¥èŒä½
                    validation_errors += 1
                    failed_count += 1
                    job_id = job_data.get("id", "unknown")
                    logger.warning(f"èŒä½ {job_id} éªŒè¯å¤±è´¥: {e.message}")
                except ScraperParsingError as e:
                    # è§£æé”™è¯¯ï¼ˆæ•°æ®è½¬æ¢å¤±è´¥ï¼‰- è·³è¿‡è¯¥èŒä½
                    parsing_errors += 1
                    failed_count += 1
                    job_id = job_data.get("id", "unknown")
                    logger.warning(f"èŒä½ {job_id} è§£æå¤±è´¥: {e.message}")
                except Exception as e:
                    # æœªçŸ¥é”™è¯¯ - è·³è¿‡è¯¥èŒä½
                    failed_count += 1
                    job_id = job_data.get("id", "unknown")
                    logger.warning(f"èŒä½ {job_id} è½¬æ¢å¤±è´¥ï¼ˆæœªçŸ¥é”™è¯¯ï¼‰: {e}")

            if failed_count > 0:
                logger.warning(
                    f"{failed_count} ä¸ªèŒä½è½¬æ¢å¤±è´¥ "
                    f"(éªŒè¯é”™è¯¯: {validation_errors}, è§£æé”™è¯¯: {parsing_errors}, "
                    f"å…¶ä»–: {failed_count - validation_errors - parsing_errors})"
                )

            # ğŸ”§ FIX: å»é‡ - åŸºäº source_id
            original_count = len(jobs)
            jobs = self._deduplicate_by_source_id(jobs)
            duplicates_removed = original_count - len(jobs)

            if duplicates_removed > 0:
                logger.warning(f"ç§»é™¤äº† {duplicates_removed} ä¸ªé‡å¤èŒä½ï¼ˆåŸºäº source_idï¼‰")

            logger.info(f"æˆåŠŸè½¬æ¢ {len(jobs)} ä¸ªèŒä½ï¼ˆå»é‡åï¼‰")
            return jobs

        except (ScraperNetworkError, ScraperTimeoutError, ScraperDataError, PlatformException):
            # è¿™äº›æ˜¯è‡´å‘½é”™è¯¯ï¼Œç›´æ¥å‘ä¸Šä¼ é€’
            raise
        except Exception as e:
            logger.error(f"SEEK æŠ“å–å¤±è´¥ï¼ˆæœªçŸ¥é”™è¯¯ï¼‰: {e}")
            raise ScraperException(
                message=f"SEEK æŠ“å–å¤±è´¥: {str(e)}",
                platform=self.platform_name,
                original_error=e
            )

    def _build_params(self, keywords: str, location: str, results_wanted: int) -> dict:
        """
        æ„å»º SEEK REST API URL å‚æ•°

        Args:
            keywords: æœç´¢å…³é”®è¯
            location: åœ°ç‚¹ï¼ˆå¦‚ Sydney, Melbourneï¼‰
            results_wanted: æœŸæœ›ç»“æœæ•°é‡

        Returns:
            dict: URL æŸ¥è¯¢å‚æ•°
        """
        # ğŸ”§ FIX (2025-12-26): åœ°ç‚¹è¿‡æ»¤ä¿®å¤
        # ä¹‹å‰ç¡¬ç¼–ç  "where": "All Australia"ï¼Œå¯¼è‡´åœ°ç‚¹è¿‡æ»¤å¤±æ•ˆ
        # ç°åœ¨ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„ location å‚æ•°
        # æµ‹è¯•ç»“æœï¼šSydney æœç´¢ 100% è¿”å› NSW èŒä½
        params = {
            "siteKey": "AU-Main",
            "where": location,  # ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„åœ°ç‚¹ï¼ˆä¿®å¤åï¼‰
            "keywords": keywords,
            "page": 1,
            "pageSize": results_wanted,
            "locale": "en-AU"
        }

        return params

    def _call_seek_api(self, params: dict) -> dict:
        """
        è°ƒç”¨ SEEK REST API

        Args:
            params: URL æŸ¥è¯¢å‚æ•°

        Returns:
            dict: API å“åº”æ•°æ®

        Raises:
            ScraperTimeoutError: è¯·æ±‚è¶…æ—¶
            ScraperNetworkError: ç½‘ç»œé”™è¯¯
            PlatformException: API è¿”å›é”™è¯¯çŠ¶æ€ç 
            ScraperDataError: å“åº”æ ¼å¼é”™è¯¯
        """
        try:
            response = requests.get(
                url=self.api_url,
                params=params,
                headers=self.headers,
                timeout=30  # 30 ç§’è¶…æ—¶
            )

            # æ£€æŸ¥ HTTP çŠ¶æ€ç 
            response.raise_for_status()

            # è§£æ JSON
            try:
                response_data = response.json()
            except ValueError as e:
                logger.error(f"SEEK API å“åº”ä¸æ˜¯æœ‰æ•ˆçš„ JSON: {e}")
                raise ScraperDataError(
                    message="API å“åº”ä¸æ˜¯æœ‰æ•ˆçš„ JSON",
                    platform=self.platform_name,
                    original_error=e
                )

            # éªŒè¯å“åº”æ ¼å¼
            if "data" not in response_data:
                logger.error("SEEK API å“åº”ç¼ºå°‘ 'data' å­—æ®µ")
                raise ScraperDataError(
                    message="API å“åº”ç¼ºå°‘ 'data' å­—æ®µ",
                    platform=self.platform_name
                )

            return response_data

        except requests.Timeout as e:
            logger.error(f"SEEK API è¶…æ—¶: {e}")
            raise ScraperTimeoutError(
                message="SEEK API è¯·æ±‚è¶…æ—¶ï¼ˆ30ç§’ï¼‰",
                platform=self.platform_name,
                original_error=e
            )
        except requests.HTTPError as e:
            logger.error(f"SEEK API HTTP é”™è¯¯: {e}")
            raise classify_http_error(
                status_code=response.status_code,
                platform=self.platform_name,
                message=f"SEEK API è¿”å›é”™è¯¯: {response.status_code}"
            )
        except requests.ConnectionError as e:
            logger.error(f"SEEK API è¿æ¥é”™è¯¯: {e}")
            raise ScraperNetworkError(
                message="æ— æ³•è¿æ¥åˆ° SEEK API",
                platform=self.platform_name,
                original_error=e
            )
        except requests.RequestException as e:
            logger.error(f"SEEK API è¯·æ±‚å¤±è´¥: {e}")
            raise ScraperNetworkError(
                message=f"SEEK API è¯·æ±‚å¤±è´¥: {str(e)}",
                platform=self.platform_name,
                original_error=e
            )

    def _deduplicate_by_source_id(self, jobs: List[JobPostingDTO]) -> List[JobPostingDTO]:
        """
        åŸºäº source_id å»é‡

        ğŸ”§ FIX (2025-12-26): Python é€‚é…å™¨å±‚å»é‡
        è§£å†³ SEEK API å¯èƒ½è¿”å›é‡å¤èŒä½çš„é—®é¢˜ï¼ˆåŒä¸€ source_id å‡ºç°å¤šæ¬¡ï¼‰

        è¿™æ˜¯ç¬¬ä¸€å±‚å»é‡ï¼ˆPython å±‚ï¼‰ï¼Œæ•°æ®åº“å±‚è¿˜æœ‰ç¬¬äºŒå±‚å»é‡ï¼ˆfingerprint + content_hashï¼‰
        åŒå±‚å»é‡ç¡®ä¿æ•°æ®è´¨é‡ï¼š
        - Python å±‚ï¼šé˜²æ­¢å•æ¬¡æŠ“å–ä¸­çš„é‡å¤ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
        - æ•°æ®åº“å±‚ï¼šé˜²æ­¢å¤šæ¬¡æŠ“å–é—´çš„é‡å¤ï¼ˆæ•°æ®å®Œæ•´æ€§ï¼‰

        Args:
            jobs: èŒä½åˆ—è¡¨

        Returns:
            List[JobPostingDTO]: å»é‡åçš„èŒä½åˆ—è¡¨
        """
        seen_ids = set()
        unique_jobs = []

        for job in jobs:
            if job.source_id not in seen_ids:
                seen_ids.add(job.source_id)
                unique_jobs.append(job)
            else:
                logger.debug(f"å‘ç°é‡å¤èŒä½: {job.source_id} - {job.title}")

        return unique_jobs

    def _extract_description(self, job_data: dict) -> Optional[str]:
        """
        ä» SEEK API æ•°æ®ä¸­æå–èŒä½æè¿°

        ä¼˜å…ˆçº§:
            1. job_data.get("teaser")         # HTML ç‰‡æ®µ
            2. job_data.get("bulletPoints")   # è¦ç‚¹åˆ—è¡¨
            3. None

        Args:
            job_data: SEEK API è¿”å›çš„èŒä½å¯¹è±¡

        Returns:
            str or None: æ¸…ç†åçš„æè¿°æ–‡æœ¬ï¼ˆé™åˆ¶ 500 å­—ç¬¦ï¼‰
        """
        # å°è¯•æå– teaser
        teaser = job_data.get("teaser")
        if teaser:
            description = clean_html(teaser)
            if description and len(description) > 500:
                description = description[:500] + "..."
            return description

        # å°è¯•æå– bulletPoints
        bullet_points = job_data.get("bulletPoints", [])
        if bullet_points:
            description = " â€¢ ".join(bullet_points)
            if len(description) > 500:
                description = description[:500] + "..."
            return description

        return None

    def _transform_job(self, job_data: dict) -> Optional[JobPostingDTO]:
        """
        å°† SEEK API è¿”å›çš„å•ä¸ªèŒä½è½¬æ¢ä¸º JobPostingDTO

        Args:
            job_data: SEEK API è¿”å›çš„èŒä½å¯¹è±¡

        Returns:
            JobPostingDTO or Noneï¼ˆè½¬æ¢å¤±è´¥æ—¶è¿”å› Noneï¼‰

        Raises:
            ScraperValidationError: ç¼ºå°‘å¿…éœ€å­—æ®µ
            ScraperParsingError: æ•°æ®è§£æå¤±è´¥
        """
        try:
            # æå–åŸºæœ¬å­—æ®µï¼ˆSEEK REST API æ ¼å¼ï¼‰
            job_id = job_data.get("id")
            title = job_data.get("title")

            # éªŒè¯å¿…éœ€å­—æ®µ
            if not job_id:
                logger.warning("èŒä½ç¼ºå°‘å¿…éœ€å­—æ®µ: id")
                raise ScraperValidationError(
                    message="èŒä½ç¼ºå°‘å¿…éœ€å­—æ®µ: id",
                    field="id",
                    platform=self.platform_name
                )

            if not title:
                logger.warning(f"èŒä½ {job_id} ç¼ºå°‘å¿…éœ€å­—æ®µ: title")
                raise ScraperValidationError(
                    message=f"èŒä½ {job_id} ç¼ºå°‘å¿…éœ€å­—æ®µ: title",
                    field="title",
                    platform=self.platform_name
                )

            # æå–å…¬å¸åç§°ï¼ˆadvertiser.descriptionï¼‰
            advertiser = job_data.get("advertiser", {})
            company = advertiser.get("description") or job_data.get("companyName") or job_data.get("employer", {}).get("name")

            # æå–åœ°ç‚¹ï¼ˆlocations æ•°ç»„çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼‰
            locations = job_data.get("locations", [])
            location_label = ""
            if locations and len(locations) > 0:
                location_label = locations[0].get("label", "")
            state, suburb = parse_location(location_label)  # parse_location è¿”å› (state, suburb)

            # æå–æè¿°
            description = self._extract_description(job_data)

            # è§£æè–ªèµ„èŒƒå›´ï¼ˆsalaryLabel å­—ç¬¦ä¸²ï¼‰
            salary_str = job_data.get("salaryLabel")
            min_amount, max_amount = parse_salary_range(salary_str)

            # æå–å·¥ä½œç±»å‹ï¼ˆworkTypes æ•°ç»„çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼‰
            work_types = job_data.get("workTypes", [])
            work_type = work_types[0] if work_types else None
            job_type = normalize_employment_type(work_type)

            # æå– trade
            trade = extract_trade(title)

            # æå–å‘å¸ƒæ—¶é—´ï¼ˆlistingDateï¼‰
            created_at = job_data.get("listingDate")

            # æ„å»ºèŒä½ URL
            job_url = f"https://www.seek.com.au/job/{job_id}"

            # è§£æå‘å¸ƒæ—¶é—´ï¼ˆå¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸º datetimeï¼‰
            posted_at = None
            if created_at:
                if isinstance(created_at, str):
                    try:
                        posted_at = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                    except ValueError as e:
                        logger.warning(f"èŒä½ {job_id} æ—¥æœŸè§£æå¤±è´¥: {created_at}, é”™è¯¯: {e}")
                        # æ—¥æœŸè§£æå¤±è´¥ä¸å½±å“å…¶ä»–æ•°æ®ï¼Œç»§ç»­å¤„ç†
                        posted_at = None
                elif isinstance(created_at, datetime):
                    posted_at = created_at

            # åˆ›å»º DTOï¼ˆä½¿ç”¨æ­£ç¡®çš„å­—æ®µåï¼‰
            try:
                return JobPostingDTO(
                    source=PlatformEnum.SEEK,
                    source_id=str(job_id),
                    title=title,
                    company=company or "Unknown",  # company æ˜¯å¿…éœ€å­—æ®µ
                    location_suburb=suburb,
                    location_state=state,
                    trade=trade,
                    employment_type=job_type,
                    pay_range_min=min_amount,
                    pay_range_max=max_amount,
                    description=description,
                    posted_at=posted_at,
                    job_url=job_url
                )
            except Exception as e:
                logger.error(f"èŒä½ {job_id} DTO åˆ›å»ºå¤±è´¥: {e}")
                raise ScraperParsingError(
                    message=f"èŒä½ {job_id} DTO åˆ›å»ºå¤±è´¥",
                    platform=self.platform_name,
                    original_error=e
                )

        except ScraperValidationError:
            # éªŒè¯é”™è¯¯ç›´æ¥å‘ä¸Šä¼ é€’
            raise
        except ScraperParsingError:
            # è§£æé”™è¯¯ç›´æ¥å‘ä¸Šä¼ é€’
            raise
        except Exception as e:
            logger.error(f"èŒä½è½¬æ¢å¤±è´¥ï¼ˆæœªçŸ¥é”™è¯¯ï¼‰: {e}")
            return None
